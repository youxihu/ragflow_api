package parsedocs

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	listdocp "ragflow_api/internal/fileManagement/listdocs"
	"time"

	"ragflow_api/internal/str"
)

type DocParser struct {
	cfg str.RagConf
}

// ParseConfig è§£æä»»åŠ¡é…ç½®
type ParseConfig struct {
	BatchSize   int           // æ¯æ‰¹æœ€å¤§æ–‡æ¡£æ•°
	MaxRetries  int           // æœ€å¤§é‡è¯•æ¬¡æ•°
	Timeout     time.Duration // å•ä¸ªæ–‡æ¡£ç­‰å¾…è¶…æ—¶æ—¶é—´
	RetryWait   time.Duration // é‡è¯•é—´éš”
	StopOnError bool          // é‡åˆ°å¤±è´¥æ˜¯å¦åœæ­¢æ•´ä¸ªæµç¨‹
}

// é»˜è®¤é…ç½®
var DefaultParseConfig = ParseConfig{
	BatchSize:   20,
	MaxRetries:  3,
	Timeout:     4 * time.Hour,
	RetryWait:   10 * time.Second,
	StopOnError: false,
}

func NewDocParser(cfg str.RagConf) *DocParser {
	return &DocParser{cfg: cfg}
}

// ParseZeroChunkDocs æŒ‰ dataset_id ä¸²è¡Œè§£ææ–‡æ¡£ï¼Œä¸€ä¸ª dataset å…¨éƒ¨è§£æå®Œæˆåæ‰å¼€å§‹ä¸‹ä¸€ä¸ª
func (p *DocParser) ParseZeroChunkDocs() error {
	cfg := DefaultParseConfig
	for _, datasetID := range p.cfg.RagFlow.DatasetID {
		log.Printf("ğŸ”„ æ­£åœ¨å¤„ç† dataset [%s]", datasetID)

		// 1. è·å–æ–‡æ¡£åˆ—è¡¨
		lister := listdocp.NewDocLister(p.cfg)
		resp, err := lister.GetListResultByDatasetID(datasetID, 1, 100, "create_time", true, "", "", "")
		if err != nil {
			log.Printf("âŒ dataset [%s] è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: %v", datasetID, err)
			if cfg.StopOnError {
				return fmt.Errorf("dataset [%s] è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: %v", datasetID, err)
			}
			continue
		}

		// 2. è¿‡æ»¤å‡ºéœ€è¦è§£æçš„æ–‡æ¡£
		var documentIDs []string
		for _, doc := range resp.Data.Docs {
			if doc.ChunkCount == 0 && !isProcessingStatus(doc.Run) {
				documentIDs = append(documentIDs, doc.ID)
				log.Printf("ğŸ“Œ dataset [%s] å‘ç°æœªè§£ææ–‡æ¡£ï¼ˆChunkCount == 0ï¼‰: %s", datasetID, doc.Name)
			} else if doc.ChunkCount > 0 && !isStableStatus(doc.Run) && !isProcessingStatus(doc.Run) {
				documentIDs = append(documentIDs, doc.ID)
				log.Printf("ğŸ“Œ dataset [%s] å‘ç°éœ€é‡æ–°è§£ææ–‡æ¡£ï¼ˆçŠ¶æ€: %sï¼‰: %s", datasetID, doc.Run, doc.Name)
			}
		}

		if len(documentIDs) == 0 {
			log.Printf("âœ… dataset [%s] æ‰€æœ‰æ–‡æ¡£å‡å·²è§£ææˆ–çŠ¶æ€æ­£å¸¸ï¼Œæ— éœ€æ“ä½œ", datasetID)
			continue
		}

		log.Printf("ğŸ“„ dataset [%s] å…±å‘ç° %d ä¸ªå¾…è§£ææ–‡æ¡£ï¼Œå°†æŒ‰æ¯æ‰¹ %d ä¸ªè¿›è¡Œå¤„ç†", datasetID, len(documentIDs), cfg.BatchSize)

		// 3. åˆ†æ‰¹æ¬¡å¤„ç†
		for i := 0; i < len(documentIDs); i += cfg.BatchSize {
			end := i + cfg.BatchSize
			if end > len(documentIDs) {
				end = len(documentIDs)
			}
			batch := documentIDs[i:end]

			log.Printf("ğŸ“¤ dataset [%s] å³å°†è§£æç¬¬ %d - %d ä¸ªæ–‡æ¡£...", datasetID, i+1, end)

			// 3.1 å†æ¬¡æ£€æŸ¥æ–‡æ¡£çŠ¶æ€
			finalBatch := p.filterNonProcessingDocs(datasetID, batch)
			if len(finalBatch) == 0 {
				log.Printf("âš ï¸ dataset [%s] ç¬¬ %d - %d ä¸ªæ–‡æ¡£éƒ½åœ¨è§£æä¸­ï¼Œè·³è¿‡æœ¬æ¬¡è¯·æ±‚", datasetID, i+1, end)
				continue
			}

			// 3.2 å°è¯•å‘é€è§£æè¯·æ±‚
			err := p.parseWithRetry(datasetID, finalBatch, cfg.MaxRetries, cfg.RetryWait)
			if err != nil {
				log.Printf("âŒ dataset [%s] ç¬¬ %d - %d ä¸ªæ–‡æ¡£å¤šæ¬¡å°è¯•å¤±è´¥ï¼Œä»æ— æ³•è§£æ: %v", datasetID, i+1, end, err)
				if cfg.StopOnError {
					return fmt.Errorf("dataset [%s] è§£æå¤±è´¥: %v", datasetID, err)
				}
				continue
			}

			// 3.3 ç­‰å¾…å®Œæˆ
			log.Printf("â³ dataset [%s] ç­‰å¾…ç¬¬ %d - %d ä¸ªæ–‡æ¡£è§£æå®Œæˆ...", datasetID, i+1, end)
			err = p.waitForDocumentsDone(datasetID, finalBatch, cfg.Timeout, 10*time.Second)
			if err != nil {
				log.Printf("âŒ dataset [%s] ç¬¬ %d - %d ä¸ªæ–‡æ¡£è§£æå¤±è´¥æˆ–è¶…æ—¶: %v", datasetID, i+1, end, err)
				if cfg.StopOnError {
					return fmt.Errorf("dataset [%s] è§£æå¤±è´¥: %v", datasetID, err)
				}
			}

			log.Printf("âœ… dataset [%s] ç¬¬ %d - %d ä¸ªæ–‡æ¡£å·²æˆåŠŸè§£æ", datasetID, i+1, end)
		}

		log.Printf("ğŸ‰ dataset [%s] æˆåŠŸå®Œæˆå…¨éƒ¨æ–‡æ¡£çš„è§£æä»»åŠ¡", datasetID)
	}

	return nil
}

// parseWithRetry åŠ å…¥é‡è¯•æœºåˆ¶
func (p *DocParser) parseWithRetry(datasetID string, docIDs []string, maxRetries int, retryWait time.Duration) error {
	for attempt := 1; attempt <= maxRetries; attempt++ {
		log.Printf("ğŸ” ç¬¬ %d/%d æ¬¡å°è¯•è§£ææ–‡æ¡£: %v", attempt, maxRetries, docIDs)
		err := p.parseDocuments(datasetID, docIDs)
		if err == nil {
			return nil
		}

		// æ‰“å°å¤±è´¥æ–‡æ¡£
		failedDocs, _ := p.getFailedDocumentIDs(datasetID, docIDs)
		log.Printf("ğŸ“Œ ç¬¬ %d æ¬¡å°è¯•å¤±è´¥ï¼Œå¤±è´¥æ–‡æ¡£: %v", attempt, failedDocs)

		if attempt < maxRetries {
			log.Printf("ğŸ•’ æ­£åœ¨ç­‰å¾… %s åé‡è¯•...", retryWait)
			time.Sleep(retryWait)
		}
	}
	return fmt.Errorf("å¤šæ¬¡å°è¯•å¤±è´¥")
}

// getFailedDocumentIDs è¿”å›å½“å‰å¤„äº RUNNING/PROCESSING çŠ¶æ€çš„æ–‡æ¡£
func (p *DocParser) getFailedDocumentIDs(datasetID string, docIDs []string) ([]string, error) {
	lister := listdocp.NewDocLister(p.cfg)
	resp, err := lister.GetListResultByDatasetID(datasetID, 1, 100, "create_time", true, "", "", "")
	if err != nil {
		return nil, err
	}

	docMap := make(map[string]string)
	for _, doc := range resp.Data.Docs {
		docMap[doc.ID] = doc.Run
	}

	var failedDocs []string
	for _, id := range docIDs {
		runStatus := docMap[id]
		if isProcessingStatus(runStatus) {
			failedDocs = append(failedDocs, id)
		}
	}

	return failedDocs, nil
}

// parseDocuments å‘ RagFlow æ¥å£å‘é€è§£æè¯·æ±‚
func (p *DocParser) parseDocuments(datasetID string, documentIDs []string) error {
	url := fmt.Sprintf(
		"http://%s:%d/api/v1/datasets/%s/chunks",
		p.cfg.RagFlow.Address,
		p.cfg.RagFlow.Port,
		datasetID,
	)

	reqBody := str.ParseRequest{
		DocumentIDs: documentIDs,
	}

	body := new(bytes.Buffer)
	if err := json.NewEncoder(body).Encode(reqBody); err != nil {
		return fmt.Errorf("æ„å»ºè¯·æ±‚ä½“å¤±è´¥: %v", err)
	}

	req, err := http.NewRequestWithContext(context.Background(), "POST", url, body)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºè¯·æ±‚å¤±è´¥: %v", err)
	}

	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", "Bearer "+p.cfg.RagFlow.APIKey)

	client := &http.Client{
		Timeout: 30 * time.Second,
	}

	resp, err := client.Do(req)
	if err != nil {
		return fmt.Errorf("å‘é€è¯·æ±‚å¤±è´¥: %v", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : %d", resp.StatusCode)
	}

	var result map[string]interface{}
	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return fmt.Errorf("JSON è§£æå¤±è´¥: %v", err)
	}

	code, ok := result["code"].(float64)
	if !ok || code != 0 {
		message := result["message"]
		return fmt.Errorf("æ¥å£è¿”å›é”™è¯¯: %v", message)
	}

	return nil
}

// isStableStatus åˆ¤æ–­æ–‡æ¡£æ˜¯å¦å¤„äºç¨³å®šçŠ¶æ€ï¼ˆæ— éœ€é‡æ–°è§£æï¼‰
func isStableStatus(status string) bool {
	switch status {
	case "DONE", "FAIL":
		return true
	default:
		return false
	}
}

// isProcessingStatus åˆ¤æ–­æ–‡æ¡£æ˜¯å¦å¤„äºè§£æè¿‡ç¨‹ä¸­
func isProcessingStatus(status string) bool {
	switch status {
	case "RUNNING", "PROCESSING", "WAITING", "PENDING":
		return true
	default:
		return false
	}
}

// waitForDocumentsDone è½®è¯¢æŒ‡å®šæ–‡æ¡£æ˜¯å¦è¿›å…¥ç¨³å®šçŠ¶æ€ï¼ˆDONE/FAILï¼‰
func (p *DocParser) waitForDocumentsDone(datasetID string, documentIDs []string, timeout, interval time.Duration) error {
	ticker := time.NewTicker(interval)
	defer ticker.Stop()

	timer := time.NewTimer(timeout)
	defer timer.Stop()

	for {
		select {
		case <-ticker.C:
			// è·å–æœ€æ–°æ–‡æ¡£ä¿¡æ¯
			lister := listdocp.NewDocLister(p.cfg)
			resp, err := lister.GetListResultByDatasetID(datasetID, 1, 100, "create_time", true, "", "", "")
			if err != nil {
				continue
			}

			running := make(map[string]bool)
			for _, doc := range resp.Data.Docs {
				if isProcessingStatus(doc.Run) {
					running[doc.ID] = true
				}
			}

			completed := true
			for _, id := range documentIDs {
				if running[id] {
					completed = false
					break
				}
			}

			if completed {
				return nil
			}

		case <-timer.C:
			statuses := make(map[string]string)
			lister := listdocp.NewDocLister(p.cfg)
			resp, _ := lister.GetListResultByDatasetID(datasetID, 1, 100, "create_time", true, "", "", "")
			for _, doc := range resp.Data.Docs {
				statuses[doc.ID] = doc.Run
			}
			return fmt.Errorf("ç­‰å¾…è¶…æ—¶ï¼Œéƒ¨åˆ†æ–‡æ¡£ä»åœ¨è¿è¡Œ: %v", statuses)
		}
	}
}

// filterNonProcessingDocs è¿‡æ»¤å‡ºä¸åœ¨è§£æçŠ¶æ€çš„æ–‡æ¡£ ID
func (p *DocParser) filterNonProcessingDocs(datasetID string, docIDs []string) []string {
	lister := listdocp.NewDocLister(p.cfg)
	resp, err := lister.GetListResultByDatasetID(datasetID, 1, 100, "create_time", true, "", "", "")
	if err != nil {
		log.Printf("âš ï¸ dataset [%s] è·å–æ–‡æ¡£çŠ¶æ€å¤±è´¥: %vï¼Œè·³è¿‡è¿‡æ»¤", datasetID, err)
		return docIDs
	}

	docMap := make(map[string]string)
	for _, doc := range resp.Data.Docs {
		docMap[doc.ID] = doc.Run
	}

	var result []string
	for _, id := range docIDs {
		runStatus := docMap[id]
		if !isProcessingStatus(runStatus) {
			result = append(result, id)
		} else {
			log.Printf("ğŸ“Œ æ–‡æ¡£ [%s] å½“å‰çŠ¶æ€ä¸º [%s]ï¼Œè·³è¿‡è§£æ", id, runStatus)
		}
	}

	return result
}
