package parsedocs

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"sync"
	"time"

	"ragflow_api/intenal/fileManagement/listdocs"
	"ragflow_api/intenal/str"
)

type DocParser struct {
	cfg str.RagConf
}

func NewDocParser(cfg str.RagConf) *DocParser {
	return &DocParser{cfg: cfg}
}

// ParseZeroChunkDocs è·å–æ¯ä¸ª dataset ä¸­ chunk_count == 0 çš„æ–‡æ¡£ï¼Œå¹¶è§¦å‘è§£æï¼ˆå¹¶å‘ç‰ˆæœ¬ï¼‰
func (p *DocParser) ParseZeroChunkDocs() error {
	var wg sync.WaitGroup
	errChan := make(chan error, len(p.cfg.RagFlow.DatasetID))

	for _, datasetID := range p.cfg.RagFlow.DatasetID {
		wg.Add(1)
		datasetID := datasetID // é¿å…é—­åŒ…é—®é¢˜

		go func() {
			defer wg.Done()

			log.Printf("ğŸ”„ æ­£åœ¨å¤„ç† dataset [%s]", datasetID)

			lister := listdocs.NewDocLister(p.cfg)

			resp, err := lister.GetListResultByDatasetID(
				datasetID,
				1, 30,
				"create_time", true,
				"", "", "",
			)
			if err != nil {
				log.Printf("âŒ dataset [%s] è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: %v", datasetID, err)
				errChan <- fmt.Errorf("dataset [%s] è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: %v", datasetID, err)
				return
			}

			var documentIDs []string
			for _, doc := range resp.Data.Docs {
				if doc.ChunkCount == 0 {
					documentIDs = append(documentIDs, doc.ID)
					log.Printf("ğŸ“Œ dataset [%s] å‘ç°æœªè§£ææ–‡æ¡£: %s", datasetID, doc.Name)
				}
			}

			if len(documentIDs) == 0 {
				log.Printf("âœ… dataset [%s] æ‰€æœ‰æ–‡æ¡£å‡å·²è§£æï¼Œæ— éœ€æ“ä½œ", datasetID)
				return
			}

			log.Printf("ğŸ“¤ dataset [%s] å³å°†è§£æ %d ä¸ªæœªå¤„ç†æ–‡æ¡£...", datasetID, len(documentIDs))
			err = p.parseDocuments(datasetID, documentIDs)
			if err != nil {
				log.Printf("âŒ dataset [%s] æ–‡æ¡£è§£æå¤±è´¥: %v", datasetID, err)
				errChan <- fmt.Errorf("dataset [%s] è§£æå¤±è´¥: %v", datasetID, err)
				return
			}

			log.Printf("ğŸ‰ dataset [%s] æˆåŠŸè§¦å‘ %d ä¸ªæ–‡æ¡£çš„è§£æä»»åŠ¡", datasetID, len(documentIDs))
		}()
	}

	wg.Wait()
	close(errChan)

	var errs []error
	for e := range errChan {
		if e != nil {
			errs = append(errs, e)
		}
	}

	if len(errs) > 0 {
		return fmt.Errorf("éƒ¨åˆ† dataset è§£æå¤±è´¥: %v", errs)
	}

	return nil
}

// parseDocuments å‘ RagFlow æ¥å£å‘é€è§£æè¯·æ±‚
func (p *DocParser) parseDocuments(datasetID string, documentIDs []string) error {
	url := fmt.Sprintf(
		"http://%s:%d/api/v1/datasets/%s/chunks",
		p.cfg.RagFlow.Address,
		p.cfg.RagFlow.Port,
		datasetID,
	)

	reqBody := str.ParseRequest{
		DocumentIDs: documentIDs,
	}

	body := new(bytes.Buffer)
	if err := json.NewEncoder(body).Encode(reqBody); err != nil {
		return fmt.Errorf("æ„å»ºè¯·æ±‚ä½“å¤±è´¥: %v", err)
	}

	req, err := http.NewRequestWithContext(context.Background(), "POST", url, body)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºè¯·æ±‚å¤±è´¥: %v", err)
	}

	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", "Bearer "+p.cfg.RagFlow.APIKey)

	client := &http.Client{
		Timeout: 30 * time.Second,
	}

	resp, err := client.Do(req)
	if err != nil {
		return fmt.Errorf("å‘é€è¯·æ±‚å¤±è´¥: %v", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : %d", resp.StatusCode)
	}

	var result map[string]interface{}
	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return fmt.Errorf("JSON è§£æå¤±è´¥: %v", err)
	}

	code, ok := result["code"].(float64)
	if !ok || code != 0 {
		message := result["message"]
		return fmt.Errorf("æ¥å£è¿”å›é”™è¯¯: %v", message)
	}

	return nil
}
